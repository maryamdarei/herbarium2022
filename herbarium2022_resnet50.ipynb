{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maryamdarei/herbarium2022/blob/main/herbarium2022_resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to premium GPUs. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to enable Premium accelerator. Subject to availability, selecting a premium GPU may grant you access to a V100 or A100 Nvidia GPU.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape dropdown. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab up to 24 hours. To enable background execution on your notebook, in the menu navigate to `Runtime > Change runtime type` and enable Background execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out these  tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/MyDrive/herbarium2022/'\n"
      ],
      "metadata": {
        "id": "BMCa2KT67yKq",
        "outputId": "4622ee3b-2529-4215-f773-50660d646884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/gdrive/MyDrive/herbarium2022"
      ],
      "metadata": {
        "id": "riCEzyxOZvCt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "iKBqdgtAZ2x_",
        "outputId": "48e23862-dfda-4465-c217-a246b5f4abd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-970843e4-6af4-4f8f-89f4-85f776ded562\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-970843e4-6af4-4f8f-89f4-85f776ded562\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mdarei\",\"key\":\"9846a03b2098a79b981d3b5b36b41766\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission"
      ],
      "metadata": {
        "id": "Dl57LBDFaRcf",
        "outputId": "70560f06-e86b-43e0-a3be-1707faf2bf4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download herbarium-2022-fgvc9"
      ],
      "metadata": {
        "id": "CJCRaQfMaZa5",
        "outputId": "d228f05b-43f4-4842-dfe6-df83a0d4c072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401 - Unauthorized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""
      ],
      "metadata": {
        "id": "RTIsvaB_M1d5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "metadata": {
        "id": "xd473EnTM7cD",
        "outputId": "204520a8-4a00-4df5-aaf9-bec3c232a001",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download c mdarei herbarium-2022-fgvc9"
      ],
      "metadata": {
        "id": "t4OnCFzRNQ-n",
        "outputId": "5149e3d7-fbde-4151-e416-31615ff0167d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle\n",
            "       [-h]\n",
            "       [-v]\n",
            "       {competitions,c,datasets,d,kernels,k,config}\n",
            "       ...\n",
            "kaggle: error: unrecognized arguments: mdarei herbarium-2022-fgvc9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n"
      ],
      "metadata": {
        "id": "_8X82OPrD6ap",
        "outputId": "7fc25ce8-e987-4ffe-cbeb-5f82f7784a81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.25.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n"
      ],
      "metadata": {
        "id": "wqxnnpN_D-n-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/\n"
      ],
      "metadata": {
        "id": "Nt-u43D7EGHn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "0QIiZa5qEKAD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import logging as log\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from matplotlib import ticker\n",
        "#I commented this:\n",
        "#from models import alexnet, mycnn, mycnn2, resnet\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import gc\n",
        "import cv2 as cv\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.metrics import f1_score\n",
        "#from tensorboardX import SummaryWriter\n",
        "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR,ReduceLROnPlateau\n",
        "from torch.utils import data\n"
      ],
      "metadata": {
        "id": "nxL4XQ0D766E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "log.basicConfig(format=\"%(asctime)s %(message)s\", level=log.INFO)\n",
        "log.info(\"======= starting to log ========\")\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "C8tN5exE8u8e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define relevant variables for the ML task\n",
        "num_classes = 15501\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5  # actual 20 epochs\n",
        "workers = 0\n",
        "pin_memory = False\n",
        "batch_size = 64\n",
        "#I commented these:\n",
        "#torch.cuda.empty_cache()\n",
        "#torch.cuda.reset_max_memory_allocated()\n",
        "#torch.cuda.synchronize()"
      ],
      "metadata": {
        "id": "hHKmDLyIAa58"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    deviceid = torch.cuda.current_device()\n",
        "    log.info(f\"Gpu device {torch.cuda.get_device_name(deviceid)}\")\n"
      ],
      "metadata": {
        "id": "Ya3Rod0hAeGa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Select the model you want to train\n",
        "\n",
        "ResNet:\n",
        "\n",
        "ResNet (Residual Network) is a type of deep neural network that was introduced in 2015 by researchers at Microsoft. The key innovation in ResNet is the use of residual connections, which allow the network to train deeper architectures without suffering from the vanishing gradient problem. Training a ResNet typically involves using stochastic gradient descent (SGD) with backpropagation to optimize the model's parameters with respect to a loss function, such as cross-entropy. This is done by iteratively updating the weights of the network in order to reduce the difference between the predicted output and the true output. Data augmentation techniques such as random cropping and flipping can be used to increase the diversity of the training set and improve the generalization of the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dYdLQeP2AhKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelname = \"RestNet50_\""
      ],
      "metadata": {
        "id": "qgjq3G58AiF9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" comented this:\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def list_files(dir):\n",
        "    r = []\n",
        "    for root, dirs, files in os.walk(dir):\n",
        "        for name in files:\n",
        "            r.append(os.path.join(root, name))\n",
        "    return r\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    paths = list_files(\"/content/drive/MyDrive/Herbarium Data-2019/Data/sample/val\")\n",
        "    problem_files =[]\n",
        "    for path in paths:\n",
        "        with open(path, 'rb') as file:\n",
        "            try:\n",
        "                img = Image.open(file).load()\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(path)\n",
        "                problem_files.append(path)\n",
        "                \"\"\"\n"
      ],
      "metadata": {
        "id": "klFz6Du-AlIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing Data, Load data"
      ],
      "metadata": {
        "id": "6O6_dksZAnfe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_img_pth_class(data_dir,split=\"train\"):\n",
        "    \"\"\" Read meta-data and change to file and class list\"\"\"\n",
        "    # 1. Load json meta data\n",
        "    with open(os.path.join(data_dir,\"{}_metadata.json\".format(split))) as f:\n",
        "        file_data = json.load(f)\n",
        "    # 2. Iterate through image and class list and save full pth and class\n",
        "    print(\"Generating {} file and class list\".format(split))\n",
        "    if split == \"train\":\n",
        "        full_file_list, class_list = [],[]\n",
        "        for i in tqdm(range(len(file_data[\"annotations\"]))):\n",
        "            # Ensure same picture\n",
        "            assert file_data[\"annotations\"][i][\"image_id\"] == file_data[\"images\"][i][\"image_id\"]\n",
        "            full_file_list.append(os.path.join(data_dir,\"train_images\",file_data[\"images\"][i][\"file_name\"]))\n",
        "            class_list.append(file_data[\"annotations\"][i][\"category_id\"])\n",
        "        # 3. Return as np array\n",
        "        return np.array(full_file_list), np.array(class_list)\n",
        "    else:\n",
        "        full_file_list = []\n",
        "        full_id_list = []\n",
        "        for i in tqdm(range(len(file_data))):\n",
        "            full_id_list.append(file_data[i]['image_id'])\n",
        "            full_file_list.append(os.path.join(data_dir,\"test_images\",file_data[i][\"file_name\"]))\n",
        "        # 3. Return as np array\n",
        "        return np.array(full_file_list), np.array(full_id_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_loaders(batch_size=32,num_workers=2): # here there was another input data_dir that I removed it because it wasn't use\n",
        "    \"\"\"Returns train and test loader\"\"\"\n",
        "    # 1. Get train data, make data set, and make data loader\n",
        "    train_file_pths, train_cls = generate_img_pth_class(\"/content/gdrive/MyDrive/herbarium2022\")# here I change the directory path\n",
        "    train_data_set = HerbariumDataLoader(train_file_pths, train_cls)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data_set,batch_size=batch_size,shuffle=True,pin_memory=True,num_workers=num_workers)\n",
        "    # 2. Calculate number of classes -- use additional 3 classes so that mapping is 1 to 1\n",
        "    num_classes = np.unique(train_cls)[-1]+1\n",
        "    \n",
        "    return train_loader, num_classes\n",
        "\n",
        "def get_transforms(train):\n",
        "    mean= [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    if train:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "    else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "    return transform\n",
        "\n",
        "class HerbariumDataLoader(data.Dataset):\n",
        "\n",
        "    def __init__(self, file_pths, class_list, train=True):\n",
        "        self.is_train_set = train\n",
        "        self.file_paths = file_pths\n",
        "        self.classes = class_list\n",
        "        self.transforms = get_transforms(train)\n",
        "        self.total_img_count = self.file_paths.shape[0]\n",
        "        self.image_size = 277 #I changed it from 380 to 277\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_img_count\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        # 1. Get class\n",
        "        class_label = np.int64(self.classes[idx]) if self.is_train_set else None\n",
        "\n",
        "        # 2. Load image, resize and transform\n",
        "        img = Image.open(self.file_paths[idx])\n",
        "        img = img.resize((self.image_size,self.image_size))\n",
        "        img = self.transforms(img)\n",
        "\n",
        "        # 3. Return image and class\n",
        "        return img, class_label\n"
      ],
      "metadata": {
        "id": "OFSrsQgcAyNh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_metadata.json"
      ],
      "metadata": {
        "id": "GWzK1eVtb2Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_img_pth_class(\"/content/gdrive/MyDrive/herbarium2022\")"
      ],
      "metadata": {
        "id": "7MenufmzA2di",
        "outputId": "f5d62cba-3eba-4d66-fbd1-0a7db088b83e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating train file and class list\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 839772/839772 [00:02<00:00, 305832.09it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['/content/gdrive/MyDrive/herbarium2022/train_images/000/00/00000__001.jpg',\n",
              "        '/content/gdrive/MyDrive/herbarium2022/train_images/000/00/00000__002.jpg',\n",
              "        '/content/gdrive/MyDrive/herbarium2022/train_images/000/00/00000__003.jpg',\n",
              "        ...,\n",
              "        '/content/gdrive/MyDrive/herbarium2022/train_images/155/04/15504__035.jpg',\n",
              "        '/content/gdrive/MyDrive/herbarium2022/train_images/155/04/15504__036.jpg',\n",
              "        '/content/gdrive/MyDrive/herbarium2022/train_images/155/04/15504__037.jpg'],\n",
              "       dtype='<U72'), array([    0,     0,     0, ..., 15504, 15504, 15504]))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, num_classes = get_loaders()"
      ],
      "metadata": {
        "id": "_HUjqzIRcUBM",
        "outputId": "ac2a6e35-fdbe-414c-da14-089960f36ad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating train file and class list\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 839772/839772 [00:02<00:00, 326850.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "id": "5jWkDdfZcZJL",
        "outputId": "fc3b229d-eed0-4201-9a64-2d77b0963a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7feadcc81bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data visualization"
      ],
      "metadata": {
        "id": "f-PLfu-O1xDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_batch(dl):\n",
        "    \"\"\"Plot images grid of single batch\"\"\"\n",
        "    for images, labels in dl:\n",
        "        fig,ax = plt.subplots(figsize = (16,20))\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
        "        break\n",
        "        \n",
        "show_batch(train_loader)\n"
      ],
      "metadata": {
        "id": "JPXb6rvA1qSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I added this line for running the model:\n",
        "model = torchvision.models.resnet50()\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "f1II-bnH11ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our optimizer and loss function\n",
        "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "lossFn = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "FuHLjsxD14pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model:\n",
        "\n",
        "Training a CNN with ResNet involves several steps:\n",
        "\n",
        "Prepare the dataset: This includes loading the images and labels, preprocessing the data (e.g. resizing, normalizing), and splitting the data into training and validation sets.\n",
        "\n",
        "Load the ResNet model: This can be done using a library such as Keras or PyTorch, which provide pre-trained ResNet models that can be easily loaded and used for transfer learning.\n",
        "\n",
        "Freeze the layers: Since ResNet is pre-trained on a large dataset, the initial layers of the model contain valuable features that can be used for the new task. So it's often a good idea to freeze the initial layers of the model and only train the last few layers, which are randomly initialized.\n",
        "\n",
        "Compile the model: This step involves specifying the optimizer, loss function, and metrics that will be used to train the model.\n",
        "\n",
        "Train the model: This involves passing the training data through the model, updating the model's parameters based on the loss function, and repeating this process for a number of epochs.\n",
        "\n",
        "Validate the model: After training, the model is evaluated on a validation dataset to estimate its performance on unseen data.\n",
        "\n",
        "Fine-tune the model: Based on the validation results, you can adjust the model's architecture, hyperparameters, or the data preprocessing steps to improve its performance.\n",
        "\n",
        "Test the model: Finally, the model is evaluated on a test dataset to get a final estimate of its performance on unseen data.\n",
        "\n",
        "It's important to note that training deep neural networks like ResNet can be computationally expensive and time-consuming. It's also important to have a good balance of dataset to avoid overfitting."
      ],
      "metadata": {
        "id": "8vrorrEw2B2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "# Train the model\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    log.info(f\"Shape of X [N, C, H, W]: {images.shape}\")\n",
        "    log.info(f\"Shape of y: {labels.shape} {labels.dtype}\")\n",
        "    # test one flow\n",
        "    # pred = model(x)\n",
        "    # loss = lossFn(pred, y)\n",
        "    break\n",
        "total_step = len(train_loader)\n",
        "log.info(f\"Total steps: {total_step}\")\n",
        "\n",
        "stepsize = total_step // 100\n",
        "if stepsize < 10:\n",
        "    stepsize = 10\n"
      ],
      "metadata": {
        "id": "UM0itfXu19LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write training matrics to Tensorboard\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# loop over our epochs\n",
        "for epoch in range(0, num_epochs):\n",
        "    # set the model in training mode\n",
        "    model.train()  # IMPORTANT otherwise the model is not in training mode\n",
        "    # initialize the total training and validation loss\n",
        "    totalTrainLoss = 0\n",
        "    totalValLoss = 0\n",
        "    # initialize the number of correct predictions in the training\n",
        "    # and validation step\n",
        "    trainAccuracy = 0\n",
        "    totalTrainAccuracy = 0\n",
        "    valCorrect = 0\n",
        "\n",
        "    # loop over the training set\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        try:\n",
        "            # Train in auto-mode with 16 bit mode\n",
        "            # with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            # Train in normal mode\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float32):\n",
        "                # send the input to the device\n",
        "                (images, labels) = (images.to(device), labels.to(device))\n",
        "                # perform a forward pass and calculate the training loss\n",
        "                outputs = model(images)\n",
        "                # output is float16 because linear layers autocast to float16.\n",
        "                # assert outputs.dtype is torch.float16 or 64\n",
        "\n",
        "                loss = lossFn(outputs, labels)\n",
        "                # zero out the gradients, perform the backpropagation step,\n",
        "                # and update the weights\n",
        "                writer.add_scalar(\"Loss/train\", loss,  (epoch * total_step)+(i+1))\n",
        "                opt.zero_grad()  # IMPORTANT otherwise the gradients of previous batches are not zeroed out\n",
        "        except Exception as e:\n",
        "            log.error(f\"Exception in data processing- skip and continue = {e}\")\n",
        "        loss.backward()\n",
        "        totalTrainLoss += loss\n",
        "        opt.step()\n",
        "        # Get the predicted values\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        trainAccuracy = (predicted == labels).float().sum().item()\n",
        "        trainAccuracy = 100 * trainAccuracy / labels.size(0)\n",
        "        writer.add_scalar(\"Accuracy/train\", trainAccuracy,(epoch * total_step)+(i+1))\n",
        "        totalTrainAccuracy += trainAccuracy\n",
        "        # if (i // stepsize) % 10 == 0:\n",
        "        log.info(\n",
        "            \"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} Accuracy: {:.4f}\".format(\n",
        "                epoch + 1, num_epochs, i + 1, total_step, loss, trainAccuracy\n",
        "            )\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "    avgTrainLoss = totalTrainLoss / len(train_loader)\n",
        "    avgAccuracy = totalTrainAccuracy / len(train_loader)\n",
        "    log.info(\n",
        "        \"--->Epoch [{}/{}], Average Loss: {:.4f} Average Accuracy: {:.4f}\".format(\n",
        "            epoch + 1, num_epochs, avgTrainLoss, avgAccuracy\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}, TrainAccuracy: {:.4f}, averageloss: {:.4f}, averageacc: {:.4f} '.format(epoch+1, num_epochs, loss, trainAccuracy, avgTrainLoss, avgAccuracy))\n",
        "\n",
        "    # End Epoch loop\n",
        "writer.flush()\n",
        "\n"
      ],
      "metadata": {
        "id": "64nplW5W2Gtu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}